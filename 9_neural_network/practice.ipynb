{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как строить нейронные сети?\n",
    "\n",
    "Нейронные сети позволяют автоматизировать подбор признаковых пространств объектов (необязательно с линейными зависимостями).\n",
    "\n",
    "Для этого как раз и используются функции активации между (линейными) слоями. Потому что без этой функции активации у нас бы линейный слой шел бы за линейным слоем. А линейная комбинация линейных слоев это все тот же **один** линейный слой. Добавив **нелинейность** мы все дальше можем уйти от линейного представления данных в более сложные объекты.\n",
    "При этом руками нужно подбирать лишь функции активации.\n",
    "\n",
    "Таким образом, мы получаем нейронную сеть, которая может описывать разные объекты с нелинейными зависимостями, все слои из которой можно дифференцировать (лучше бы, для метода обратного распространения ошибки и поиска градиента)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск и метод обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нелинейные преобразования:**\n",
    "* $ f(a) = \\dfrac{1}{1 + e^a} $ -- sigmoid, сигмоида\n",
    "* $ f(a) =  \\tanh (a) $ -- гиперболические тангенс\n",
    "* $ f(a) = \\max (0, a) $ -- ReLU. Недифференцируемая. Но если рассматривать, что слева $f'(+0) = 0$, справа $f'(-0) = 1$. То есть производную посчитать можно. Функция не обязана быть *гладкой*.\n",
    "* $ f(a) = \\log(1 + e^a) $ -- softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим нейронные сети. Базовые понятия.\n",
    "**Слой** -- структурная единица нейронки, которая включает в себя линейное преобразование + нелинейная функция активации, которая применяется к результату линейного слоя.\n",
    "\n",
    "Входной слой -- исходное представление данных.\n",
    "Выходной слой -- итоговое представление данных: метка класса, вероятность класса, предсказанное число.\n",
    "\n",
    "Все обучается с помощью **backpropagation, метода обратного распространения ошибки** -- методу обучения сложных функций.\n",
    "\n",
    "Производная сложной функции:\n",
    "$$ \\dfrac{\\delta L}{\\delta x} = \\dfrac{\\delta L}{\\delta z} \\cdot \\dfrac{\\delta z}{\\delta x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В многомерном случае:\n",
    "![Метод распространения ошибок от $f(x, y$](pics/nn_gradients.png)\n",
    "\n",
    "![](pics/div_complicated_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Градиент -- направление наискорейшего *возрастания* функции.\n",
    "\n",
    "Обучение моделей машинного обучения мы строим на минимизации функции ошибок. И для минимизации этой *loss-функции* мы используем метод градиентного спуска, то есть двигаемся в сторону антиградиента.\n",
    "\n",
    "Нашу нейронную сеть можно представить как очень сложную функцию. Потому находить градиент нейронки мы также можем находить как производную сложной функции -- \"дифференцировать\" *шаг за шагом*.\n",
    "\n",
    "Пример с картинки:\n",
    "\n",
    "$$ \\dfrac{\\delta L}{\\delta x} = \\dfrac{\\delta L}{\\delta z} \\cdot  \\left(\\dfrac{\\delta L}{\\delta x} + \\dfrac{\\delta L}{\\delta y}\\right)$$\n",
    "\n",
    "Все эти свистопляски можно представить как *граф вычислений*:\n",
    "![](pics/example-gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полносвязная нейронная сеть\n",
    "\n",
    "- относительно хорошо справляется с классификацией\n",
    "- по факту куча линейных слоев\n",
    "\n",
    "![](pics/mlp_classification.png)\n",
    "![](pics/mlp_penultimate_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На последнем слое, чтоб превратить эти меры (не от 0 до 1) в вероятности принадлежности к классу применяется softmax-преобразование.\n",
    "\n",
    "Необходимо, что сумма вероятностей принадлежности к определенному классу давала 1.\n",
    "\n",
    "Преобразование softmax:\n",
    "$$\n",
    "    softmax(y_i) = \\dfrac{e^{y_i}}{\\sum e^{y_i}} \\in  [0, 1], \\\\\n",
    "    \\sum softmax(y_i) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь для последнего слоя:\n",
    "$$ logloss(y, p) = \\log p_y(x) \\rightarrow \\max $$\n",
    "или в общем виде функции потерь\n",
    "$$ -logloss(y, p) = -\\log p_y(x) \\rightarrow \\min $$\n",
    "\n",
    "Итоговая задача оптимизации:\n",
    "$$ \\dfrac{1}{l} \\sum logloss(y, p(x, \\theta)) \\rightarrow \\min_\\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастический градиентный спуск\n",
    "* Выбираем батч из обучающей выборки\n",
    "* Обновляем веса backpro на этом батче, а не на всей выборке\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/neuron_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сигмоида\n",
    "$$ f(a) = \\dfrac{1}{1 + e^a} $$\n",
    "+ отображение в [0, 1]\n",
    "+ интерпретируется как вероятность\n",
    "+ дифференцируема\n",
    "- обладает \"хвостами\", в которых производная почти 0\n",
    "- тяжелая частная производная, тяжело считать\n",
    "- затухание градиента\n",
    "- нет центрирования \n",
    "* используется в *реккурентных* нейронных сетях\n",
    "\n",
    "### Гиперболический тангенс\n",
    "$$ f(a) = \\tanh(a) $$\n",
    "+ отображение в [-1, 1]\n",
    "+ центрированно\n",
    "- затухание градиента на хвостах\n",
    "- все еще надо считать экспоненту\n",
    "* используется в *реккурентных* нейронных сетях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU, \n",
    "+ отображение в [0, $+\\inf$]\n",
    "+ намного легче высчитывать и самую функцию\n",
    "+ производную намного легче считать: либо 0, либо 1\n",
    "- не центрированно в нуле\n",
    "- слева на хвосте затухают градиенты\n",
    "\n",
    "### Leaky ReLU\n",
    "$$f(x) = \\max(0.01x, x)$$\n",
    "+ не затухает\n",
    "+ легко вычисляется\n",
    "+ простая производная\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric PReLU\n",
    "$$f(x) = \\max(\\alpha x, x)$$\n",
    "* просто почему бы и нет\n",
    "* параметр альфа можно настраивает с помощью backprop\n",
    "\n",
    "### ELU\n",
    "$$\n",
    "    f(x) = \\begin{cases}\n",
    "        x, \\quad x > 0 \\\\\n",
    "        \\alpha (\\exp(x) - 1), \\quad x \\leq 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "+ все плюсы ReLU\n",
    "+ не затухает\n",
    "- нужно считать экспоненту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/activation_functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция активации выбирается в соответствии от типа нейронных сетей: реккурентные, сверточные и так далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/advices_activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
